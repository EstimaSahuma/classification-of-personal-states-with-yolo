# üëÅÔ∏è EyeState Detector: An√°lise de Aten√ß√£o em Tempo Real

![Python](https://img.shields.io/badge/Python-3.9%2B-blue.svg)
![OpenCV](https://img.shields.io/badge/OpenCV-4.x-green.svg)
![YOLOv8](https://img.shields.io/badge/YOLO-v8-orange.svg)
![License](https://img.shields.io/badge/License-MIT-brightgreen.svg)

**EyeState Detector** √© um projeto de vis√£o computacional avan√ßado que analisa o estado de aten√ß√£o de uma pessoa (focada, distra√≠da, cansada, etc.) em tempo real, utilizando apenas a imagem do olho capturada por uma webcam.

Sua principal aplica√ß√£o √© na **seguran√ßa veicular**, onde pode ser usado para monitorar motoristas e gerar alertas caso sinais de distra√ß√£o ou sonol√™ncia sejam detectados, ajudando a prevenir acidentes de tr√¢nsito.

*(Sugest√£o: grave um GIF curto mostrando o programa funcionando e coloque aqui)*

---

## üìù √çndice

- [üìå Sobre o Projeto](#-sobre-o-projeto)
- [‚öôÔ∏è Como Funciona](#Ô∏è-como-funciona)
- [üöÄ Instala√ß√£o e Execu√ß√£o](#-instala√ß√£o-e-execu√ß√£o)
- [üõ†Ô∏è Tecnologias Utilizadas](#Ô∏è-tecnologias-utilizadas)
- [ü§ù Como Contribuir](#-como-contribuir)
- [üìù Licen√ßa](#-licen√ßa)

---

## üìå Sobre o Projeto

O objetivo do **EyeState Detector** √© criar um sistema robusto e n√£o invasivo para inferir o estado cognitivo de uma pessoa. Ao inv√©s de analisar express√µes faciais complexas, o projeto se concentra nos micro-movimentos e padr√µes do olho, que s√£o fortes indicadores de aten√ß√£o e fadiga.

O sistema classifica o estado do usu√°rio em categorias como:
- **Focado (Focus)**
- **Distra√≠do (Distracted)**
- **Cansado (Tired)**
- E outros estados que podem ser treinados.

A tecnologia central √© uma combina√ß√£o de detec√ß√£o de pontos-chave (keypoints) e uma abordagem bio-inspirada que simula uma **c√¢mera de eventos (Event-based Camera)**, tornando o sistema mais resiliente a varia√ß√µes de ilumina√ß√£o.

## ‚öôÔ∏è Como Funciona

O pipeline do projeto √© dividido em v√°rias etapas inteligentes que trabalham em conjunto para fornecer uma an√°lise est√°vel e precisa:

1.  **Detec√ß√£o do Olho com YOLOv8-Pose**:
    - Utilizamos o modelo `yolov8n-pose.pt` para detectar os pontos-chave (keypoints) do corpo humano na imagem da webcam.
    - Focamos especificamente no keypoint do olho esquerdo (`LEFT_EYE_INDEX = 2`) para obter uma localiza√ß√£o precisa.

2.  **Estabiliza√ß√£o e Rastreamento**:
    - Para evitar que a caixa de detec√ß√£o (bounding box) ao redor do olho "pule" a cada frame, aplicamos um **filtro de m√©dia m√≥vel exponencial**. Isso suaviza a posi√ß√£o do centro do olho, resultando em um rastreamento mais est√°vel.
    - Um mecanismo de persist√™ncia mant√©m a caixa de detec√ß√£o vis√≠vel por alguns frames mesmo se o olho for temporariamente perdido (ex: durante uma piscada).

3.  **Convers√£o para C√¢mera de Eventos (V2E - Video to Events)**:
    - Este √© o cora√ß√£o do projeto. Ao inv√©s de alimentar o classificador com a imagem RGB crua do olho, n√≥s a processamos com a classe `V2E_Simulator_Pro`.
    - Esta classe simula o funcionamento de uma c√¢mera de eventos, que captura apenas as **mudan√ßas de luminosidade** em cada pixel.
    - Ela gera uma imagem em preto, branco e cinza, onde:
        - **Branco**: Pixels que ficaram mais claros (eventos "ON").
        - **Preto**: Pixels que ficaram mais escuros (eventos "OFF").
        - **Cinza**: Nenhuma mudan√ßa significativa.
    - Essa abordagem torna a classifica√ß√£o muito mais robusta a varia√ß√µes absolutas de ilumina√ß√£o e foca nos padr√µes de movimento do olho.

4.  **Classifica√ß√£o do Estado com YOLOv8-Clf**:
    - A imagem de eventos gerada na etapa anterior √© ent√£o enviada para um segundo modelo YOLOv8, treinado especificamente para **classifica√ß√£o de imagens**.
    - Este modelo (`best.pt`) foi treinado com um dataset de imagens de eventos de olhos para aprender a diferenciar os estados de "focado", "distra√≠do", etc.
    - O desempenho do classificador no conjunto de valida√ß√£o pode ser visualizado na **matriz de confus√£o normalizada** abaixo. Ela mostra a capacidade do modelo de distinguir corretamente entre as diferentes classes.

    ![Matriz de Confus√£o Normalizada](https://github.com/Netinhoklz/classification-of-personal-states-with-yolo/blob/main/train412/confusion_matrix_normalized.png?raw=true)
    
    *Como podemos observar, os valores na diagonal principal s√£o altos, indicando uma alta taxa de acerto para cada classe. As confus√µes entre estados opostos (ex: `Focus` e `Tired`) s√£o m√≠nimas, validando a efic√°cia da abordagem.*

5.  **Sistema de Vota√ß√£o para Estabilidade**:
    - Classifica√ß√µes frame a frame podem ser vol√°teis. Para garantir um resultado final confi√°vel, implementamos um **sistema de vota√ß√£o**.
    - As √∫ltimas `75` predi√ß√µes s√£o armazenadas em uma fila (`deque`).
    - O estado final exibido ao usu√°rio √© a **moda** (a classe mais frequente) dessa janela de tempo, prevenindo "flickering" entre estados e fornecendo uma leitura muito mais est√°vel e confi√°vel.

6.  **Visualiza√ß√£o em Tempo Real**:
    - O resultado √© exibido na tela com:
        - Uma caixa colorida ao redor do olho (Verde para foco, Vermelho para distra√ß√£o).
        - O status atual e a confian√ßa da vota√ß√£o.
        - Um cron√¥metro que registra o tempo gasto em cada estado.
        - Uma janela separada que mostra a vis√£o da "c√¢mera de eventos", permitindo visualizar exatamente o que o classificador est√° "vendo".

## üöÄ Instala√ß√£o e Execu√ß√£o

Siga os passos abaixo para executar o projeto em sua m√°quina local.

### Pr√©-requisitos
- Python 3.9 ou superior
- Git
- Uma webcam conectada

### Passos

1.  **Clone o reposit√≥rio:**
    ```bash
    git clone https://github.com/Netinhoklz/classification-of-personal-states-with-yolo.git
    cd classification-of-personal-states-with-yolo
    ```

2.  **Crie e ative um ambiente virtual (recomendado):**
    ```bash
    # Windows
    python -m venv venv
    .\venv\Scripts\activate

    # macOS / Linux
    python3 -m venv venv
    source venv/bin/activate
    ```

3.  **Instale as depend√™ncias:**
    Crie um arquivo `requirements.txt` com o seguinte conte√∫do:
    ```txt
    opencv-python
    numpy
    ultralytics
    Pillow
    ```
    E ent√£o instale-as:
    ```bash
    pip install -r requirements.txt
    ```

4.  **Baixe os modelos YOLO:**
    - O `yolov8n-pose.pt` ser√° baixado automaticamente pela biblioteca `ultralytics` na primeira execu√ß√£o.
    - **IMPORTANTE**: O modelo classificador treinado (`best.pt`) j√° deve estar no reposit√≥rio ou voc√™ precisa baix√°-lo. **Certifique-se de que o caminho no script est√° correto**:

    ```python
    # Altere esta linha no seu c√≥digo, se necess√°rio
    model_classifier = YOLO(r'C:\caminho\completo\para\o\seu\best.pt') 
    ```

5.  **Execute o script:**
    ```bash
    python seu_script.py
    ```
    *(Substitua `seu_script.py` pelo nome do seu arquivo Python principal)*

Pressione a tecla `q` com a janela do OpenCV em foco para fechar o programa.

## üõ†Ô∏è Tecnologias Utilizadas

- **Python**: Linguagem de programa√ß√£o principal.
- **OpenCV**: Para captura de v√≠deo e manipula√ß√£o de imagens em tempo real.
- **Ultralytics YOLOv8**: Para detec√ß√£o de pose e classifica√ß√£o de imagens, utilizando redes neurais de √∫ltima gera√ß√£o.
- **NumPy**: Para computa√ß√£o num√©rica e manipula√ß√£o de arrays de imagem.
- **Pillow (PIL)**: Para renderizar texto com fontes customizadas na tela.

---

## ü§ù Como Contribuir

Contribui√ß√µes s√£o o que tornam a comunidade de c√≥digo aberto um lugar incr√≠vel para aprender, inspirar e criar. Qualquer contribui√ß√£o que voc√™ fizer ser√° **muito apreciada**.

1.  Fa√ßa um **Fork** do projeto.
2.  Crie uma **Branch** para sua feature (`git checkout -b feature/AmazingFeature`).
3.  Fa√ßa o **Commit** de suas mudan√ßas (`git commit -m 'Add some AmazingFeature'`).
4.  Fa√ßa o **Push** para a Branch (`git push origin feature/AmazingFeature`).
5.  Abra um **Pull Request**.

---

## üìù Licen√ßa

Distribu√≠do sob a licen√ßa MIT. Veja `LICENSE` para mais informa√ß√µes.
